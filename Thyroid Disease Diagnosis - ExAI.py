# -*- coding: utf-8 -*-
"""Thyroid_Disease_Detection_Without_Sampling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KWCrAn_pd-Axa35qVsp-Q_6dHYTmBrNl
"""



"""## **Description:**

Thyroid disease refers to a medical condition that affects the thyroid gland, a butterfly-shaped gland located in the front of the neck. The thyroid gland plays a crucial role in producing hormones that regulate various bodily functions, including metabolism, growth, and development.

There are several types of thyroid diseases, including:

**1. Hypothyroidism:** This occurs when the thyroid gland does not produce enough thyroid hormones. Symptoms may include fatigue, weight gain, sensitivity to cold, depression, and slowed heart rate.

**2. Hyperthyroidism:** In contrast to hypothyroidism, hyperthyroidism refers to an overactive thyroid gland that produces excessive amounts of thyroid hormones. Symptoms may include weight loss, rapid heartbeat, irritability, tremors, and heat intolerance.

**3. Thyroid nodules:** Thyroid nodules are lumps or abnormal growths that form within the thyroid gland. Most nodules are noncancerous (benign) and do not cause noticeable symptoms. However, some nodules can be cancerous or produce excess thyroid hormones.

**4. Thyroiditis:** Thyroiditis is inflammation of the thyroid gland, which can lead to temporary or long-term disruption of thyroid hormone production. It can be caused by viral infections, autoimmune conditions, or certain medications.

**5. Thyroid cancer:** Thyroid cancer is a relatively uncommon form of cancer that originates in the thyroid gland. It is typically treatable and has a high survival rate if detected early.

Thyroid disease can have various causes, including autoimmune disorders (e.g., Hashimoto's thyroiditis, Graves' disease), genetic factors, radiation exposure, certain medications, and iodine deficiency.


The most common blood tests used to diagnose thyroid disease are:

* **Thyroid-stimulating hormone (TSH) test:** This test measures the level of TSH in the blood. TSH is a hormone produced by the pituitary gland that stimulates the thyroid gland to produce thyroid hormones. A high TSH level can indicate hypothyroidism, while a low TSH level can indicate hyperthyroidism.
* **Free T4 test:** This test measures the level of free T4 in the blood. Free T4 is the unbound form of T4, which is the most active form of thyroid hormone. A low free T4 level can indicate hypothyroidism, while a high free T4 level can indicate hyperthyroidism.
* **Free T3 test:** This test measures the level of free T3 in the blood. Free T3 is the unbound form of T3, which is another active form of thyroid hormone. A low free T3 level can indicate hypothyroidism, while a high free T3 level can indicate hyperthyroidism.

Other blood tests that may be ordered include:

* **Thyroid peroxidase antibodies (TPOAb) test:** This test measures the level of TPOAb in the blood. TPOAb are antibodies that attack the thyroid gland. A high level of TPOAb can indicate an autoimmune thyroid disease, such as Hashimoto's thyroiditis.
* **Thyroglobulin antibodies (TgAb) test:** This test measures the level of TgAb in the blood. TgAb are antibodies that attack thyroglobulin, a protein produced by the thyroid gland. A high level of TgAb can indicate an autoimmune thyroid disease, such as Graves' disease.

Imaging tests that may be ordered to diagnose thyroid disease include:

* **Thyroid ultrasound:** This test uses sound waves to create images of the thyroid gland. This can be used to look for nodules, cysts, or other abnormalities in the thyroid gland.
* **Thyroid scan:** This test uses a small amount of radioactive iodine to create images of the thyroid gland. This can be used to look for areas of the thyroid gland that are not functioning properly.

Physical examinations can also be helpful in diagnosing thyroid disease. The doctor will look for signs of hypothyroidism, such as weight gain, fatigue, and cold intolerance. The doctor will also look for signs of hyperthyroidism, such as weight loss, anxiety, and heat intolerance.

# Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.utils import resample
from imblearn.over_sampling import SMOTENC,RandomOverSampler,KMeansSMOTE
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import RandomOverSampler

from scipy.stats import skew


import pickle

from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from statsmodels.api import add_constant


# import the train_test_split library
from sklearn.model_selection import train_test_split

# importing the SMOTE library
from imblearn.over_sampling import SMOTE

# ML classifiers
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics


# performance parameters
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

"""# Reading Datasets

## Sort Information Of Datasets:

Here are the features of the datasets *allhypo.data* and *allhyper.data*, which are related to thyroid disease:

* **Age:** Age in years
* **Sex:** Gender (M or F)
* **On Thyroxine:** If the patient is currently taking thyroxine (1 or 0)
* **Thyroid Status:** Whether the patient has hypothyroid (1) or hyperthyroid (0) disease
* **TSH:** Thyroid-stimulating hormone level in milli-international units per liter (mIU/L)
* **T4:** Free thyroxine level in nanograms per deciliter (ng/dL)
* **T3:** Free triiodothyronine level in nanograms per deciliter (ng/dL)
* **FTI:** Free thyroxine index
* **TBG:** Thyroxine-binding globulin level in milligrams per deciliter (mg/dL)
* **RT3U:** Reverse triiodothyronine level in nanograms per deciliter (ng/dL)
"""

df_hypo = pd.read_csv('/content/allhypo.data', header = None, index_col = False)
df_hyper = pd.read_csv('/content/allhyper.data', header = None, index_col = False)

df_hypo.head()

df_hypo.shape

df_hypo.columns.unique()

df_hyper.head()

df_hyper.shape

"""# Data Processing

**This code will read the two thyroid datasets into Pandas DataFrames. The column names are defined in the columns variable. The read_csv() function is used to read the data from the files. The na_values parameter is used to specify the values that represent missing data. The index_col parameter is used to specify the column that should be used as the index.**
"""

# Import the necessary libraries
import pandas as pd

# Define the column names
columns = ["age", "sex", "on thyroxine", "query on thyroxine", "on antithyroid medication", "sick", "pregnant",
           "thyroid surgery", "I131 treatment", "query hypothyroid", "query hyperthyroid", "lithium", "goitre",
           "tumor", "hypopituitary", "psych", "TSH measured", "TSH", "T3 measured", "T3", "TT4 measured", "TT4",
           "T4U measured", "T4U", "FTI measured", "FTI", "TBG measured", "TBG", "referral source", "labels"]

# Read the hypothyroid data into a Pandas DataFrame
df_hypo = pd.read_csv("/content/allhypo.data", names=columns, na_values=["?"], index_col=False)

# Read the hyperthyroid data into a Pandas DataFrame
df_hyper = pd.read_csv("/content/allhyper.data", names=columns, na_values=["?"], index_col=False)

# Print the column names of the hypothyroid DataFrame
print(df_hypo.columns)

# Print the first few rows of the hypothyroid DataFrame
print(df_hypo.head())

# Print the column names of the hyperthyroid DataFrame
print(df_hyper.columns)

# Print the first few rows of the hyperthyroid DataFrame
print(df_hyper.head())

"""* Splitting the labels column at | into two columns: 'class' and 'id'.
* After that dropping the labels column.
"""

# Split the labels column into two columns: hypo_class and id
df_hypo[['hypo_class','id']] = df_hypo.labels.str.split("|",expand = True)

# Drop the labels column
df_hypo.drop('labels', axis = 1, inplace = True)

# Split the labels column into two columns: hyper_class and id
df_hyper[['hyper_class','id']] = df_hyper.labels.str.split("|",expand = True)

# Drop the labels column
df_hyper.drop('labels', axis = 1, inplace = True)

# Print the hypothyroid DataFrame
print(df_hypo.columns)

# Print the hyperthyroid DataFrame
print(df_hyper.columns)

"""* Replacing the '.' in 'class' column with space ('')

"""

df_hypo['hypo_class'] = df_hypo['hypo_class'].str.replace('.', '')
df_hyper['hyper_class'] = df_hyper['hyper_class'].str.replace('.', '')

"""* Let's make Deep copying both the datasets for future reference."""

df_hypo_copy = df_hypo.copy(deep=True)
df_hyper_copy = df_hyper.copy(deep=True)

"""* Let's Check unique value of `hypo_calss` and `hyper_class`"""

print(df_hypo['hypo_class'].unique())
print(df_hyper['hyper_class'].unique())

"""* **Since we are classifying only in three categories: `negative`, `hypothyroid`, and `hyperthyroid`.**

* let's replace other values with 'negative', 'hypothyroid', and 'hyperthyroid'.
"""

df_hypo.replace(['compensated hypothyroid', 'primary hypothyroid',\
                 'secondary hypothyroid'], 'hypothyroid', inplace = True)

df_hyper.replace(['T3 toxic', 'goitre'], 'hyperthyroid', inplace = True)

"""* **Since all values are common in both the datasets expect 'class'.**
* **So, we can concatnate both the datasets.**
"""

df_concat = pd.concat([df_hypo, df_hyper.iloc[:, -2]], axis=1)
df_concat.head()

df_concat.shape

"""* let's make deep copy of concat datasets"""

df_new = df_concat.copy(deep = True)

"""Now, the conditions based on which we are replacing the 'class' value with any of the 'class' i.e. 'hypo_class' or 'hyper_class'

- neg + neg = neg
- hypo + neg = hypo
- hyper + neg = hyper
"""

df_new.to_csv('df_new.csv', index = False)

df_concat['labels'] = np.where(((df_concat['hypo_class'] != 'negative') & \
                                (df_concat['hyper_class'] == 'negative')), df_concat['hypo_class'],\
                      np.where((df_concat['hyper_class'] != 'negative'), df_concat['hyper_class'],'negative'))

df_concat.head()

"""* Lebels Added now feature is 33

* **Dopping those columns (features) which are not relevant and storing it in a new dataframe.**
"""

df = df_concat.drop(['referral source', 'TBG', 'hypo_class', 'id', 'hyper_class'], axis = 1)

df.info()

"""Some columns are just indicating whether next column in the same row has some value or not. Like 'TSH measured' have 'true' & 'false' value.

The 'true' means, the next column in the same row has some value and 'false' means, next column in the same row has 'NaN'. So, we are any ways going to handle the missing values, there is no point of having such columns in our dataset.

Let's drop these feature columns.

For Example
"""

df[['TSH measured', 'TSH']].tail(10)

df = df.drop(['TSH measured','T3 measured','TT4 measured','T4U measured',\
              'FTI measured','TBG measured'], axis = 1)

df.head()

"""# Types Of Datasets

- categorical
    - nominal (order not matter)
    - ordinal (order matter)

- numerical
    - discrete (discrete data is counted)
    - continuous (continuous data is measured)

* let print categorical featues
"""

cat_features = [i for i in df.columns if (df[i].dtype == 'O')]
cat_features

len(cat_features)

"""* Numerical Features"""

num_features = [i for i in df.columns if (df[i].dtype != 'O')]
num_features

for i in cat_features:
    print('==='*20)
    print(i, 'feature: unique values: ', df[i].unique())

print('==='*20)

for i in num_features:
    print('==='*12)
    print(i, 'feature: unique values: ', len(df[i].unique()))

print('==='*12)

for i in num_features:
    print('==='*12)
    print(i, 'feature: minimum value: ', min(df[i].unique()))
    print(i, 'feature: maximum value: ', max(df[i].unique()))

print('==='*12)

"""#### Obeservations:
Categorical features: 15
- all are nominal data

Numerical features: 6
- discrete data: 1
- continous data: 5

#### Let's see some more insights of Categorical and Numerical Data
"""

df.describe()

# categorical data
df.describe(include = 'object')

"""### Handling Null Value"""

df.isnull().sum()

"""#### Observations:
- age has maximum value 455 which is not possible (an outlier)
- each categorical features has two unique value except labels which has three unique values
- NULL values:
    - age: 1
    - sex: 110
    - TSH: 284
    - T3: 585
    - TT4: 184
    - T4U: 297
    - FTI: 295

#### Let's handle NaN value in numerical column(s)
"""

numerical_null = [i for i in df.columns if (df[i].dtype != 'O' and df[i].isnull().sum() != 0)]
numerical_null

df[df['age'].isnull()]

"""Replacing 'NaN' value withe median of the 'age' column

"""

df['age'].replace([np.nan],df['age'].median(), inplace = True)

"""Let's check the numerical columns again.

"""

numerical_null = [i for i in df.columns if (df[i].dtype != 'O' and df[i].isnull().sum() != 0)]
numerical_null

"""Filling the 'NaN' values with 'median' of that column in all the numerical columns.

"""

for i in numerical_null:
   df[i].fillna(df[i].median(),inplace = True)

df.isnull().sum()

"""#### Let's handle NaN value in categorical column(s)

Seperating the 'object' and 'int' or 'float' columns and storing 'object' into 'categorical'.

"""

categorical = [i for i in df.columns if df[i].dtype == 'O']
categorical

categorical_null = [i for i in df.columns if (df[i].dtype == 'O' and df[i].isnull().sum() != 0)]
categorical_null

"""## HANDLING NULL VALUES OF SEX USING LOGISTIC REG"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder

# Assuming 'gender' and 'target' are columns in your dataset
# Replace 'target' with the actual column name you're trying to predict
# and 'gender' with the actual gender column name
target_col = 'labels'
gender_col = 'sex'

# Separate data with null values and non-null values
data_with_null = df[df['sex'].isnull()]
data_without_null = df[df['sex'].notnull()]

# Encode gender column using LabelEncoder
le = LabelEncoder()
# data_without_null[gender_col] = le.fit_transform(data_without_null[gender_col])
for i in categorical:
    data_without_null[i] = le.fit_transform(data_without_null[i])

data_without_null.head()

for i in categorical:
    data_with_null[i] = le.fit_transform(data_with_null[i])
data_with_null.head()

# Split data_without_null into features (X) and target (y)
X = data_without_null.drop('sex', axis=1)
y = data_without_null['sex']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# Initialize and train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict null values using the trained model
X_null = data_with_null.drop('sex', axis=1)
predicted_null_values = model.predict(X_null)

# Fill in the null values in the original dataset with predicted values
df.loc[df['sex'].isnull(), 'sex'] = predicted_null_values

df.isnull().sum()

df.isnull().sum().sum()

df['sex'].unique()

# replace 0 in sex with 'F' and 1 with 'M'

df['sex'] = df['sex'].replace([0,1],['F','M'])

df['sex'].unique()

"""* No null value presents

# Data Transformation
"""

numerical_all = [i for i in df.columns if (df[i].dtype != 'O')]
numerical_all

df_filled = df.copy(deep = True)

plt.style.use('dark_background')

def checkPlot(dataframe, feat):

    m = []


#     print("-------LOG Transformation-------")

    log_target = np.log1p(dataframe[feat])
    df_filled['log_'+i] = pd.DataFrame(log_target)


    plt.rcParams["figure.figsize"] = 13,5
    fig,ax = plt.subplots(1,2)
    sns.distplot(dataframe[feat], label= "Orginal Skew:{0}".format(np.round(skew(dataframe[feat]),4)), color="r", ax=ax[0], axlabel="ORGINAL")
    sns.distplot(log_target, label= "Transformed Skew:{0}".format(np.round(skew(log_target),4)), color="b", ax=ax[1], axlabel="LOG TRANSFORMED")
    fig.legend()
    m.append(np.round(skew(log_target),4))


#     print("-------Square Root Transformation-------")

    sqrrt_target = dataframe[feat]**(1/2)
    df_filled['sqrroot_'+i] = pd.DataFrame(sqrrt_target)

    plt.rcParams["figure.figsize"] = 13,5
    fig,ax = plt.subplots(1,2)
    sns.distplot(dataframe[feat], label= "Orginal Skew:{0}".format(np.round(skew(dataframe[feat]),4)), color="r", ax=ax[0], axlabel="ORGINAL")
    sns.distplot(sqrrt_target, label= "Transformed Skew:{0}".format(np.round(skew(sqrrt_target),4)), color="b", ax=ax[1], axlabel="SQUARE ROOT TRANSFORMED")
    fig.legend()
    m.append(np.round(skew(sqrrt_target),4))

    print(m)

import warnings
warnings.filterwarnings('ignore')

for i in df_filled[numerical_all]:

    print(" Plots after transformations for col : ",  i)

    checkPlot(df_filled, i)

"""#### Observations:

- After Applying Transforamtion and Calculating the Skewness, we found best result for numerical features:
- Age: **SQRT**
- TSH: **LOG**
- T3:  **SQRT**
- TT4: **SQRT**
- T4U: **LOG**
- FTI: **SQRT**

"""

df_transformed = df_filled.copy(deep = True)

df_transformed.columns

df_transformed.shape

"""#### Since, only the above transforamtion are useful in our case, we will drop others

#### Now Dropping transformed columns which are not useful in our case
"""

df_transformed.drop(['age', 'log_age', \
                     'TSH', 'sqrroot_TSH', \
                     'T3', 'log_T3', \
                     'TT4', 'log_TT4', \
                     'T4U', 'sqrroot_T4U', \
                     'FTI', 'log_FTI'], axis = 1, inplace = True)

df_transformed.columns

df_transformed.shape

"""### Data Encoding

Let's extract the Categorical features.
"""

df_transformed_cat = df_transformed.select_dtypes(include = ['object','category'])
df_transformed_cat.columns

df_transformed_cat.shape

df_onehot_encoded = pd.get_dummies(df_transformed_cat.iloc[:,:-1], drop_first = True)

df_onehot_encoded.columns

df_onehot_encoded.shape

"""Now, let's do the label encoding on the target/labels columns (dependent feature)"""

df_transformed_label = pd.DataFrame(df_transformed_cat.iloc[:,-1])
df_transformed_label.head()

from sklearn import preprocessing

# label_encoder object knows how to understand word labels.
label_encoder_random_forest = preprocessing.LabelEncoder()

# Encode labels in column 'labels'.
df_transformed_label['labels']= label_encoder_random_forest.fit_transform(df_transformed_label)

# we will save the encoder as pickle to use when we do the prediction. We will need to decode the predcited values
# back to original

file = "label_encoder_random_forest.pickle"
pickle.dump(label_encoder_random_forest, open(file, "wb"))

df_transformed_label['labels'].unique()

list (label_encoder_random_forest.inverse_transform([2, 1, 0]))

# ['negative', 'hypothyroid', 'hyperthyroid']

df_transformed_label.head()

df_onehot_encoded.head()

"""Let's concatnate the categorical features with encoded label"""

df_encoded_cat = pd.concat([df_onehot_encoded, df_transformed_label], axis = 1)
df_encoded_cat.head()

"""### Multi-collinearity

Few suggestions from people:

- If we are using linear Model to solve problem, we have to deal with Multicollinearity
- And Linear models are Model which creates a line to predict or separate
- Example Linear Models: SVM, Logistic Regression, Linear Regression

- If you're going to solve using decision tree or any other tree model then no need to deal with Multicollinearity

- And to check Multicollinearity - Scatter plot, correlation and VIF (between independent features)

- For classification problems - checking independent vs dependent feature is not useful

- Multicollinearity is entirely for independent features

#### Let's check and handle the Multi-collinearity in Numerical features
"""

df_numerical = df_transformed.select_dtypes(include = np.number)
df_numerical.head()

"""**Ways to check Multi-collinearity:** Scatter plot, Correlation and Variance Inflation Factor (VIF) - between independent features

**Can you calculate VIF for categorical variables?**
- VIF cannot be used on categorical data. Statistically speaking, it wouldn't make sense.
- If you want to check independence between 2 categorical variables you can however run a Chi-square test.
"""

def calculate_vif(X):

    # Calculating VIF

    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [ variance_inflation_factor(X.values, i) for i in range(X.shape[1]) ]

    return(vif)

"""#### Some VIF Rules:

- VIF starts at 1 and has no upper limit

- VIF = 1, no correlation between the independent variable and the other variables

- VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others

- VIF = 1 → No correlation

- VIF = 1 to 5 → Moderate correlation

- VIF > 10 → High correlation
"""

calculate_vif(df_numerical)

"""#### Observations:
    
These features are having high correlation:
- sqrroot_age
- sqrroot_T3
- sqrroot_TT4
- log_T4U
- sqrroot_FTI

#### Now, let's handle this carefully. Explore what is happening in behind the scene of the VIF!

#### Let's deep dive what does it means?

First understood this:

- The statsmodel API assumes line passes through origin. Hence intercept is 0.

- In that case If we try to build a linear regression model using statsmodel model our intercept will always be 0.

- So, we externally have to tell statsmodel API that our intercept should not be 0. And that's why we have to add constant in our dataset.

- If we do linear regression via SKlib, then it internally add constant. But in statsmodel, we have to do this externally.

Now coming to VIF.

- Internally VIF is nothing but a Linear regression.

- So, in the backend, each independent model is predicted using other independent model.

- E.g. if we have X, Y, Z columns in the dataset.

- Y & Z are used as independent features and X become dependent feature.

- Similarly, X & Y will act as independent features. This 2 will predict Z.

- And similarly, X & Z will act as independent features. They 2 will predict Y.

- This means VIF is using Linear Regression and this VIF is present in statsmodel API.

- And in statsmodel API, we need to externally add a constant variable in the dataset.
"""

df_numerical_constant = df_numerical.copy(deep = True)

df_numerical_constant['constant'] = 1

## adding 1 constant in our dataset.

df_numerical_constant.head()

calculate_vif(df_numerical_constant)

"""We can see the VIF is significantly reduced after adding the constant value.

Now, let's analyze the above one by dropping the highest VIF value first.
"""

df_numerical_constant.drop('sqrroot_TT4', axis = 1, inplace = True)

calculate_vif(df_numerical_constant)

"""#### Observations:
- Now we can see the VIF is significantly reduced after dropping the sqrroot_TT4 feature with highest VIF.
- And none of the features are correlated now.

This looks good.

#### Now, let's create the final dataset with numerical features by dropping the constant column
"""

df_numerical_final = df_numerical_constant.drop('constant', axis = 1)

df_numerical_final.head()

# final dataset with numerical features

"""#### Now, let's create the final dataset with categorical features"""

df_categorical_final = df_encoded_cat.copy(deep = True)

df_categorical_final.head()

# final dataset with categorical features

"""#### Cancatenate the numerical and categorical features and create a final dataset"""

df_final = pd.concat([df_numerical_final, df_categorical_final], axis = 1)

# final dataset - 2800x21

df_final.head()

df_final.columns

# checking the total value of each target class
# negative - 2
# hyporthyroid - 1
# hyperthyroid - 0

df_final.labels.value_counts()

df_final.columns

df.columns

# renaming the column name

df_final.rename({'sqrroot_age': 'age', 'log_TSH': 'TSH', 'sqrroot_T3': 'T3', \
                 'log_T4U': 'T4U', 'sqrroot_FTI' : 'FTI', 'sex_M' : 'sex', \
                 'on thyroxine_t' : 'onthyroxine', 'query on thyroxine_t' : 'queryonthyroxine', \
                 'on antithyroid medication_t' : 'onantithyroidmedication', \
                 'sick_t' : 'sick', 'pregnant_t' : 'pregnant', 'thyroid surgery_t' : 'thyroidsurgery', \
                 'I131 treatment_t' : 'I131treatment', 'query hypothyroid_t' : 'queryhypothyroid', \
                 'query hyperthyroid_t' : 'queryhyperthyroid', 'lithium_t' : 'lithium', \
                 'goitre_t' : 'goitre', 'tumor_t' : 'tumor', 'hypopituitary_t' : 'hypopituitary', \
                 'psych_t' : 'psych'}, axis = 1, inplace = True)

df_final.head()

df_final.columns

df_temp = df_final.copy(deep = True)

column_names = ["age", "sex", "TSH", "T3", "T4U", "FTI", \
                "onthyroxine", "queryonthyroxine", "onantithyroidmedication", \
                "sick", "pregnant", \
                "thyroidsurgery", "I131treatment", "queryhypothyroid", "queryhyperthyroid", \
                "lithium", "goitre", "tumor","hypopituitary","psych", "labels"]

# arranging the features in its origin sequence

df_final = df_final.reindex(columns = column_names)

df_final.to_csv('Data.csv', index = False)

df_final['labels'].value_counts()

df_final

df_final.columns

# plot a correlation matrix using seaborn with whitw background

plt.figure(figsize=(20,20))
sns.heatmap(df_final.corr(), annot=True, cmap="RdYlGn")
plt.show()

"""## improvements:

0) correlation matrix and some EDA
1) try using some method to improve data without sampling
2) use Explainable AI
3) contact doctors
4) smart correlation
5) transfer learning
6) Deploy streamlit : Drop-down: Logistics, Random forest


https://feature-engine.trainindata.com/en/1.0.x/selection/SmartCorrelatedSelection.html
"""

import pandas as pd

data = pd.read_csv("/content/Data.csv")
data

from sklearn.model_selection import train_test_split

X = data.drop(["labels"], axis = 1)
Y = data["labels"]
X_train, X_test, y_train, y_test = train_test_split(X,Y, random_state=0)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

"""##Random Forest"""

# importing the important libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# create an instance of a Random Forest classifier
random_forest_model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)

# training the Random Forest model on the sampled train dataset
random_forest_model.fit(X_train, y_train)

# pedicting the labels on the test set
y_predicted_randomforest = random_forest_model.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay

accuracy = accuracy_score(y_test, y_predicted_randomforest)
print("Accuracy:", accuracy)

import pickle

pickle.dump(random_forest_model, open("random_forest_model.pkl","wb"))

# y_pred = best_rf.predict(X_test)

# Create the confusion matrix
cm = confusion_matrix(y_test, y_predicted_randomforest)

ConfusionMatrixDisplay(confusion_matrix=cm).plot();

"""##Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

Decision_tree_model = DecisionTreeClassifier(criterion="entropy", max_depth=5)

# Train Decision Tree Classifer
Decision_tree_model.fit(X_train,y_train)

#Predict the response for test dataset
y_predicted_decisiontree = Decision_tree_model.predict(X_test)

print("Accuracy:",accuracy_score(y_test, y_predicted_decisiontree))

feature_cols = ['age', 'sex', 'TSH', 'T3', 'T4U', 'FTI', 'onthyroxine',
       'queryonthyroxine', 'onantithyroidmedication', 'sick', 'pregnant',
       'thyroidsurgery', 'I131treatment', 'queryhypothyroid',
       'queryhyperthyroid', 'lithium', 'goitre', 'tumor', 'hypopituitary',
       'psych', ]

from six import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
import pydotplus
dot_data = StringIO()
export_graphviz(Decision_tree_model, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True, feature_names = feature_cols,class_names=['0','1','2'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('tree.png')
Image(graph.create_png())

pickle.dump(Decision_tree_model, open("decision_tree_model_final.pkl","wb"))

"""## EXI"""

!pip install interpret

from sklearn.ensemble import RandomForestClassifier
from interpret.blackbox import LimeTabular
from interpret import show
import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

from lime import lime_tabular

!pip install lime

from lime import lime_tabular

# from interpret.blackbox import LimeTabular

# lime = LimeTabular(model=random_forest_model, data=X_train, random_state=1)

import numpy as np

values = np.unique(y_test)
values

values = values.astype('<U7')

columns = list(X_train.columns)
columns

x_train_np = np.array(X_train)
x_test_np = np.array(X_test)
y_train_np = np.array(y_train)
y_test_np = np.array(y_test)

target =[0,1,2]

X_train.head()

# lime_local = lime.explain_local(X_test, y_test, name=lime)
# show(lime_local)

explainer = lime_tabular.LimeTabularExplainer(x_train_np, mode="classification", class_names=target, feature_names=columns)
explainer

len(x_test_np), len(y_test_np)

import random

idx = random.randint(1, len(x_test_np)-1)
# idx = 55

print("Prediction : ", target[random_forest_model.predict(x_test_np[idx].reshape(1,-1))[0]])
print("Actual :     ", target[y_test_np[idx]])

explanation = explainer.explain_instance(x_test_np[idx], random_forest_model.predict_proba, top_labels=3)

explanation.show_in_notebook()

import numpy as np

preds = random_forest_model.predict(x_test_np)

false_preds = np.argwhere((preds != y_test_np)).flatten()

idx  = random.choice(false_preds)


print("Prediction : ", target[random_forest_model.predict(x_test_np[idx].reshape(1,-1))[0]])
print("Actual :     ", target[y_test_np[idx]])

explanation = explainer.explain_instance(x_test_np[idx], random_forest_model.predict_proba, top_labels=3)

fig = explanation.show_in_notebook()

from interpret.glassbox import ClassificationTree
model = ClassificationTree()
model.fit(X_train,y_train)
from sklearn.metrics import classification_report
pred = model.predict(X_test)
print(classification_report(y_test,pred))

from interpret import show
model_show=model.explain_local(X_test[:100], y_test[:100])
show(model_show)

model_show=model.explain_global(name="DecisionTree")
show(model_show)

model_show=model.explain_local(X_test[:100], y_test[:100], name="ExplainableBoostingClassifier")
show(model_show)

"""# SHAP"""

import shap
explainer = shap.TreeExplainer(Decision_tree_model)

shap_values = explainer.shap_values(X_test)

shap.initjs()

prediction = Decision_tree_model.predict(X_test)
print(f"The RF predicted: {prediction}")
shap.force_plot(explainer.expected_value[1], shap_values[1], X_test)

"""# Stacking (Decision Tree, Kneighbors, SVC)

"""

from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

learners = [
    ('svc', SVC(probability=True)),
    ('dt', DecisionTreeClassifier(max_depth=3)),
    ('knn', KNeighborsClassifier(n_neighbors=5))
]

stacking_model = StackingClassifier(estimators=learners, final_estimator=LogisticRegression()
)

stacking_model.fit(X_train, y_train)

y_pred = stacking_model.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

explainer = lime_tabular.LimeTabularExplainer(x_train_np, mode="classification", class_names=target, feature_names=columns)
explainer

import random

idx = random.randint(1, len(x_test_np)-1)
# idx = 55

print("Prediction : ", target[stacking_model.predict(x_test_np[idx].reshape(1,-1))[0]])
print("Actual :     ", target[y_test_np[idx]])

explanation = explainer.explain_instance(x_test_np[idx], stacking_model.predict_proba, top_labels=3)

explanation.show_in_notebook()

from sklearn.svm import SVC
classifier = SVC(kernel='rbf', random_state = 1)
classifier.fit(X_train,y_train)
Y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,Y_pred)
print("Accuracy:",accuracy_score(y_test, Y_pred))

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred  =  classifier.predict(X_test)

print("Accuracy:",accuracy_score(y_test, y_pred))

